{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CIFAR10 data\n",
    "The data can be loaded directly from keras (`keras.datasets.cifar10`).\n",
    "\n",
    "```python\n",
    "cifar10 = keras.datasets.cifar10\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from matplotlib import pyplot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 13s 0us/step\n"
     ]
    }
   ],
   "source": [
    "cifar10 = keras.datasets.cifar10\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# one hot encode target values\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "train_encoded = encoder.fit_transform(train_labels.reshape(-1,1))\n",
    "test_encoded = encoder.transform(test_labels.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  scale values with np\n",
    "\n",
    "train_scaled = np.interp(train_images, (train_images.min(), train_images.max()), (-1,+1))\n",
    "test_scaled = np.interp(test_images, (test_images.min(), test_images.max()), (-1,+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Build a convulution neural network (CNN) to predict the object in the images. \n",
    "\n",
    "Try to do it on your own first before consulting with peers or tutorials on the internet. If you are stuck early, reach out to a mentor who will point you in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-19 17:01:04.757981: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Convolution2D(16, 3, 3, input_shape = (32, 32, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(128, activation = 'relu'))\n",
    "classifier.add(Dense(10, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow(train_scaled,    \n",
    "                                                 batch_size = 32,)\n",
    "\n",
    "test_set = test_datagen.flow(test_scaled,\n",
    "                                            batch_size = 32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')    # Builds the static computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "25/25 [==============================] - 2s 67ms/step - loss: 2.0706 - accuracy: 0.2607 - val_loss: 1.8670 - val_accuracy: 0.3433\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 1s 58ms/step - loss: 1.7762 - accuracy: 0.3797 - val_loss: 1.6810 - val_accuracy: 0.4162\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 1s 57ms/step - loss: 1.6375 - accuracy: 0.4285 - val_loss: 1.5781 - val_accuracy: 0.4477\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 1s 57ms/step - loss: 1.5476 - accuracy: 0.4572 - val_loss: 1.5121 - val_accuracy: 0.4652\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 1s 56ms/step - loss: 1.4833 - accuracy: 0.4780 - val_loss: 1.4607 - val_accuracy: 0.4854\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 1s 56ms/step - loss: 1.4295 - accuracy: 0.4972 - val_loss: 1.4167 - val_accuracy: 0.4951\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 1s 60ms/step - loss: 1.3857 - accuracy: 0.5134 - val_loss: 1.3834 - val_accuracy: 0.5122\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 1s 59ms/step - loss: 1.3529 - accuracy: 0.5269 - val_loss: 1.3621 - val_accuracy: 0.5200\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 2s 68ms/step - loss: 1.3231 - accuracy: 0.5376 - val_loss: 1.3420 - val_accuracy: 0.5259\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 1s 53ms/step - loss: 1.2993 - accuracy: 0.5459 - val_loss: 1.3184 - val_accuracy: 0.5330\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 1s 52ms/step - loss: 1.2772 - accuracy: 0.5551 - val_loss: 1.3014 - val_accuracy: 0.5430\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 1s 54ms/step - loss: 1.2603 - accuracy: 0.5591 - val_loss: 1.2942 - val_accuracy: 0.5451\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 1s 53ms/step - loss: 1.2478 - accuracy: 0.5631 - val_loss: 1.2888 - val_accuracy: 0.5473\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 1s 55ms/step - loss: 1.2345 - accuracy: 0.5682 - val_loss: 1.2674 - val_accuracy: 0.5543\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 1s 54ms/step - loss: 1.2153 - accuracy: 0.5751 - val_loss: 1.2570 - val_accuracy: 0.5585\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 1s 53ms/step - loss: 1.2069 - accuracy: 0.5779 - val_loss: 1.2512 - val_accuracy: 0.5644\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 1s 53ms/step - loss: 1.1925 - accuracy: 0.5850 - val_loss: 1.2413 - val_accuracy: 0.5654\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 1s 54ms/step - loss: 1.1847 - accuracy: 0.5874 - val_loss: 1.2385 - val_accuracy: 0.5640\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 1s 54ms/step - loss: 1.1726 - accuracy: 0.5908 - val_loss: 1.2416 - val_accuracy: 0.5645\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 1s 53ms/step - loss: 1.1624 - accuracy: 0.5943 - val_loss: 1.2238 - val_accuracy: 0.5677\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 1s 59ms/step - loss: 1.1527 - accuracy: 0.5980 - val_loss: 1.2190 - val_accuracy: 0.5703\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 1s 54ms/step - loss: 1.1426 - accuracy: 0.6019 - val_loss: 1.2175 - val_accuracy: 0.5723\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 1s 52ms/step - loss: 1.1344 - accuracy: 0.6049 - val_loss: 1.2107 - val_accuracy: 0.5752\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 1s 55ms/step - loss: 1.1266 - accuracy: 0.6075 - val_loss: 1.2099 - val_accuracy: 0.5739\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 1s 53ms/step - loss: 1.1201 - accuracy: 0.6096 - val_loss: 1.2082 - val_accuracy: 0.5743\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 1s 55ms/step - loss: 1.1150 - accuracy: 0.6110 - val_loss: 1.2135 - val_accuracy: 0.5764\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 1s 55ms/step - loss: 1.1100 - accuracy: 0.6127 - val_loss: 1.1970 - val_accuracy: 0.5797\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 2s 64ms/step - loss: 1.0988 - accuracy: 0.6176 - val_loss: 1.1911 - val_accuracy: 0.5819\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 2s 62ms/step - loss: 1.0874 - accuracy: 0.6219 - val_loss: 1.1914 - val_accuracy: 0.5799\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 1s 57ms/step - loss: 1.0834 - accuracy: 0.6232 - val_loss: 1.1928 - val_accuracy: 0.5790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa267b82190>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "classifier.fit(train_scaled, train_encoded, validation_data=(test_scaled, test_encoded), epochs=30, batch_size=2000, \n",
    "          callbacks=[EarlyStopping(patience=5)], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3dccde306ff19bbc2be532b0f4481ce43cb993410f44c2d3ada687c482b579c8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
